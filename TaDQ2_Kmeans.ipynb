{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 K-means Clustering\n",
        "Isaac Tabb\n",
        "\n",
        "Text As Data\n",
        "\n",
        "24/01/2023\n"
      ],
      "metadata": {
        "id": "dpsU4oabWZ28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -1: Read in Data"
      ],
      "metadata": {
        "id": "o3pzVWJIYelX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the file from my local drive and then reads the file in as as Pandas DataFrame."
      ],
      "metadata": {
        "id": "d9H50nR_ZgAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "# upload file from computer\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io \n",
        "# read csv into df\n",
        "nbatweetsdf = pd.read_csv(io.BytesIO(uploaded['training_set.csv']))"
      ],
      "metadata": {
        "id": "DtxdfepHYR85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "049c8722-af65-43fa-f3e6-e5fe1dcda4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3cd803a8-375e-4660-9dfe-d6913d30d362\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3cd803a8-375e-4660-9dfe-d6913d30d362\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training_set.csv to training_set.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's make sure the data frame makes sense. It shows up with an unnamed row for index, we're gonna delete that one."
      ],
      "metadata": {
        "id": "nYKElkb4ZmAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbatweetsdf = nbatweetsdf.drop('Unnamed: 0', axis=1)\n",
        "# turn df to dictionary\n",
        "nbatweets = nbatweetsdf.to_dict('records')"
      ],
      "metadata": {
        "id": "HgKK2HnDZpA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Vectorize Text\n"
      ],
      "metadata": {
        "id": "YVhGI5eWZb8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing Spacy."
      ],
      "metadata": {
        "id": "LmQj5AT_aHaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6fpshEoWPka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb869895-cab1-478d-fadb-841a2b7b38d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's create our spacy pipeline."
      ],
      "metadata": {
        "id": "MtXCXPh6aNLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_pipeline(tweet):\n",
        "    tokens = []\n",
        "    doc = nlp(tweet)\n",
        "    for i in doc:\n",
        "        # make sure no stopwords, spaces, or punctuation are kept\n",
        "        if (not i.is_stop) and (not i.is_space) and (not i.is_punct):\n",
        "            tokens.append(i.lemma_.lower())\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "vK5_ioPlaM5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's print the first tweet row to make sure the dictionary looks right.\n"
      ],
      "metadata": {
        "id": "rp9cCseVfWbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nbatweets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foSdKWq5fkQI",
        "outputId": "90cd4d1a-b96c-4458-ece8-8541695c35ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@Lakers @KingJames @AntDavis23 Thanks, @bakersfieldnow &amp; @Jon_Singh19.                       üóûüì∫üïôüèÄ', 'team': 'MiamiHeat', 'date_label': 'period3', 'follower_label': 'medium'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It worked! Now we are going to create our corpus."
      ],
      "metadata": {
        "id": "08KmR2ZTfvpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create our corpus, a list of the tweet text\n",
        "corpus = []\n",
        "for tweet in nbatweets:\n",
        "  corpus.append(tweet['text'])"
      ],
      "metadata": {
        "id": "v4Dfrcqs5boT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's define out vectorizer using sklearn."
      ],
      "metadata": {
        "id": "DlITH-E4JTWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def tfidf_vectorize_with_sklearn_and_spacy(text_corpus):\n",
        "  # the vectorizer which uses the space pipeline to tokenize\n",
        "  vectorizer = TfidfVectorizer(tokenizer = spacy_pipeline)\n",
        "  # fit and transform on our corpus\n",
        "  X = vectorizer.fit_transform(text_corpus)\n",
        "  return X"
      ],
      "metadata": {
        "id": "X0LpfHUSJSTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run our new function on the tweets corpus and print the length and size of matrix.\n"
      ],
      "metadata": {
        "id": "Do_MFAoGJjn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the vectorization using spacy\n",
        "tweets_tfidf_matrix = tfidf_vectorize_with_sklearn_and_spacy(corpus)\n",
        "tweets_tfidf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1WKsofCJqHM",
        "outputId": "2cdd169a-ec28-4285-c518-f3d904f94ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6000x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 51278 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's just assert that our matrix makes sense."
      ],
      "metadata": {
        "id": "5lJaWTFdjDrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len(posts)={len(nbatweets)}\")\n",
        "print(f\"post_tfidf_matrix.shape={tweets_tfidf_matrix.shape}\")\n",
        "\n",
        "assert len(nbatweets) == tweets_tfidf_matrix.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vrrb7-sK1gn",
        "outputId": "d5461fe6-4295-4be1-af59-64f893b08290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(posts)=6000\n",
            "post_tfidf_matrix.shape=(6000, 9958)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Pick k random centroids"
      ],
      "metadata": {
        "id": "CfZHbCK5WzHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way that I will be finding random initial centroids is choosing K random documents in the dataset."
      ],
      "metadata": {
        "id": "AIMG74qWjqv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rd\n",
        "\n",
        "# define a function which takes the list of tweets and a value k\n",
        "# the function chooses k random indexes to be out centroids\n",
        "def k_random_centroids(tweets, k):\n",
        "  # pick k random indexes, no repeats\n",
        "  rand_centroid_indexes = rd.sample(range(len(nbatweets)-1), k)\n",
        "  rand_centroids = []\n",
        "  # retrieve the k random documents to be our centroids\n",
        "  for i in rand_centroid_indexes:\n",
        "    rand_centroids.append(tweets_tfidf_matrix[i,:])\n",
        "  # return the starting centroids\n",
        "  return rand_centroids\n",
        "\n",
        "# initialize k as 5 for 5 clusters\n",
        "k = 5\n",
        "# retrieve the centroids\n",
        "centroids = k_random_centroids(nbatweets, k)\n",
        "print(centroids)\n"
      ],
      "metadata": {
        "id": "d6SH77xPW3Un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75308b1-0756-4672-a922-b909e07543e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<1x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 15 stored elements in Compressed Sparse Row format>, <1x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 8 stored elements in Compressed Sparse Row format>, <1x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 15 stored elements in Compressed Sparse Row format>, <1x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 15 stored elements in Compressed Sparse Row format>, <1x9958 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 10 stored elements in Compressed Sparse Row format>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Assign each vector to its closest centroid"
      ],
      "metadata": {
        "id": "uSJgNpD4W4Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can assign all the vectors to centroids."
      ],
      "metadata": {
        "id": "Gtibmpoat-wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a function to assign each vector to a centroid, it takes the index\n",
        "# of the tweet, the current centroids list, and the tfidf matrix\n",
        "def assign_vector_to_centroid(tweet_index, centroids, tweets_tfidf_matrix):\n",
        "  # initialize a list to hold a tweets distance score from each centroid\n",
        "  scores = []\n",
        "\n",
        "  # iterate through the five centroids\n",
        "  for centroid in centroids:\n",
        "    # score the dot product\n",
        "    dotprod = centroid.dot(tweets_tfidf_matrix[tweet_index,:].T)\n",
        "    # save sim score\n",
        "    sim_score = dotprod[0,0]\n",
        "    # append the score to the scores list for each centroid\n",
        "    scores.append(sim_score)\n",
        "\n",
        "  # whichever score is the highest is the closest centroid\n",
        "  high_score = max(scores)\n",
        "\n",
        "  # get that index and return it, this is the cluster which the tweet is in\n",
        "  index_of_closest = scores.index(high_score)\n",
        "  return index_of_closest\n",
        "\n",
        "# cluster counts is used to keep track of how many tweets are in each cluster\n",
        "clus_counts = [0] * k\n",
        "\n",
        "from tqdm import tqdm\n",
        "# iterate through tweets\n",
        "for tweet in tqdm(range(len(nbatweets))):\n",
        "  # assign each tweet to a cluster\n",
        "  nbatweets[tweet]['cluster'] = assign_vector_to_centroid(tweet, centroids, tweets_tfidf_matrix)\n",
        "  # increment the cluster counts\n",
        "  clus_counts[nbatweets[tweet]['cluster']] += 1\n"
      ],
      "metadata": {
        "id": "i-oUgSLwW8Z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7427189a-8320-4bd3-b4b7-b661807f525d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Recalculate the centroids based on the closest vectors"
      ],
      "metadata": {
        "id": "1CKrIXpmW9MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I define a function recalculate the centroids."
      ],
      "metadata": {
        "id": "M35vE8JDHq8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# below is a function which recalculates each of the centroids\n",
        "# this function takes the list of nbatweets, the cluster counts, and k\n",
        "def recalculate_centroids(nbatweets, clus_counts, k):\n",
        "  # create a new list of centroids, same length as last\n",
        "  rec_centroids = []\n",
        "  for i in range(k):\n",
        "    rec_centroids.append({})\n",
        "\n",
        "  # iterate through the nbatweets\n",
        "  for tweet in tqdm(range(len(nbatweets))):\n",
        "    # retreieve the cluster the tweet is in\n",
        "    clus = nbatweets[tweet]['cluster']\n",
        "    # retrieve the rows and columns of the current tweet's sparse matrix row\n",
        "    # https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements\n",
        "    rows, cols = tweets_tfidf_matrix[tweet,:].nonzero()\n",
        "    # iterate through each one of the tokens\n",
        "    for col in cols:\n",
        "      # if the token is already in the current dictionary for the new centroid\n",
        "      if col in rec_centroids[clus]:\n",
        "        # increment that token's item by the current value\n",
        "        rec_centroids[clus][col] += tweets_tfidf_matrix[tweet,:][0,col]\n",
        "      # else the token has not been included yet so set it to the current value\n",
        "      else:\n",
        "        rec_centroids[clus][col] = tweets_tfidf_matrix[tweet,:][0,col]\n",
        "    \n",
        "  # iterate through the centroids\n",
        "  for centroid in range(len(rec_centroids)):\n",
        "    # iterate through key value pairs in current centroid\n",
        "    for key, item in rec_centroids[centroid].items():\n",
        "      # average the values by how many tweets were in that cluster\n",
        "      rec_centroids[centroid][key] = item / clus_counts[centroid]\n",
        "\n",
        "  # this list will hold the final recalculated centroids\n",
        "  final_rec_centroids = []\n",
        "  # convert dictionary back to sparse matrix\n",
        "  for centroid in rec_centroids:\n",
        "    matrix = csr_matrix((1, tweets_tfidf_matrix.shape[1]))\n",
        "    for key, item in centroid.items():\n",
        "      matrix[0, key] = item\n",
        "    \n",
        "    # final list of sparse matrices\n",
        "    final_rec_centroids.append(matrix)\n",
        "\n",
        "  # return the new average centroids\n",
        "  return final_rec_centroids\n"
      ],
      "metadata": {
        "id": "CmOKtdZZXDu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will call the recalculate centroids function on the tweets."
      ],
      "metadata": {
        "id": "8R7tm3e3H6VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec_centroids = recalculate_centroids(nbatweets, clus_counts, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpowF7qzIA9u",
        "outputId": "04cfb261-d8c2-4a31-b391-74c37c897f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 816.79it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Repeat Step 2 & 3 until converge"
      ],
      "metadata": {
        "id": "FC0aZHR6XEJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function which calculates if two lists of matrices are equal."
      ],
      "metadata": {
        "id": "1M5fV0nLlSDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function takes in recalculated centroids and original centroids and\n",
        "# checks if the lists have converged\n",
        "def check_converge(recs, cents):\n",
        "  # will count matching centroids\n",
        "  count_matching = 0\n",
        "  # iterate through past centroids\n",
        "  for cent in cents:\n",
        "    # iterate through current centroids\n",
        "    for rec in recs:\n",
        "      # iterate thru centroids and find how many of the centroids are equal between the two lists\n",
        "      # https://stackoverflow.com/questions/30685024/check-if-two-scipy-sparse-csr-matrix-are-equal\n",
        "      if (cent != rec).nnz==0:\n",
        "        count_matching += 1\n",
        "  \n",
        "  # if the number of matching centroids is greater or equal to the number of centroids\n",
        "  # then the two lists are equal\n",
        "  if count_matching >= len(cents):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "zQirm5NilWjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will run Step 4."
      ],
      "metadata": {
        "id": "iR_KY2hnl4xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the recalculated centroids are equal to the last pass centroids\n",
        "conv = check_converge(rec_centroids, centroids)\n",
        "\n",
        "# while the recalculated centroids and the old ones don't converge\n",
        "while not(conv):\n",
        "  # set the centroids list to the new recalculated centroids\n",
        "  centroids = rec_centroids\n",
        "\n",
        "  # repeat steps 2 and 3 below\n",
        "  clus_counts = [0] * k\n",
        "  for tweet in tqdm(range(len(nbatweets))):\n",
        "    nbatweets[tweet]['cluster'] = assign_vector_to_centroid(tweet, centroids, tweets_tfidf_matrix)\n",
        "    clus_counts[nbatweets[tweet]['cluster']] += 1\n",
        "  \n",
        "  rec_centroids = recalculate_centroids(nbatweets, clus_counts, k)\n",
        "  conv = check_converge(rec_centroids, centroids)\n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "id": "qvV2OBSrXHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e39975f-71ef-49a4-9d58-6b4cac9e03eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 482.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 770.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 488.62it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 714.93it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 491.49it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 752.97it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.62it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:11<00:00, 501.70it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 486.91it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 722.57it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 482.86it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 734.67it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 479.57it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 755.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 487.77it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 742.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 483.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 726.89it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 483.32it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 766.07it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 484.14it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:06<00:00, 864.02it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 486.20it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 847.77it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 490.16it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 808.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.98it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 833.23it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 491.46it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 810.96it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:11<00:00, 500.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 793.73it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 494.66it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 744.94it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 481.96it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 706.94it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 490.63it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 730.83it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 484.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 740.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 488.14it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 730.80it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.81it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 717.32it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 486.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 722.47it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 490.00it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 753.96it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.82it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 817.15it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 491.90it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 808.51it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 483.88it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 816.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 486.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 821.44it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 486.58it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 827.59it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 493.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 792.06it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:11<00:00, 500.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 760.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 484.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:08<00:00, 733.52it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:12<00:00, 485.62it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:07<00:00, 751.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clus_counts)"
      ],
      "metadata": {
        "id": "XAGVqKoRcK8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbd659e-f2f4-464e-dcdc-a8890cf690e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2107, 619, 1046, 667, 1561]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking at the distribution"
      ],
      "metadata": {
        "id": "B6NZ5oESlPor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some of the documents in the clusters."
      ],
      "metadata": {
        "id": "FI_UjIYorx-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 40 tweets, examining which clusters they are in\n",
        "for tweet in range(0, 40):\n",
        "  print(nbatweets[tweet]['text'])\n",
        "  print(nbatweets[tweet]['cluster'])"
      ],
      "metadata": {
        "id": "U2wgU9iprxNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the top words in each cluster."
      ],
      "metadata": {
        "id": "GSKkXmGGsrRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, define a method to find the five max magnitudes for a certain centroid."
      ],
      "metadata": {
        "id": "tDYRX4v8zA9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def second_elem(e):\n",
        "    return e[1]\n",
        "\n",
        "# this function finds the five maxes of the centroid list\n",
        "def five_maxes(rec_centroid, k):\n",
        "  temp = rec_centroid\n",
        "  maxes = []\n",
        "  # gets all of the nonzero rows and columns\n",
        "  rows, cols = rec_centroid[0,:].nonzero()\n",
        "  # iterates through nonzero tokens\n",
        "  for col in cols:\n",
        "    # if maxes hasnt been filled yet automatically append\n",
        "    if len(maxes) < 5:\n",
        "      maxes.append([col, rec_centroid[0, col]])\n",
        "    # if there are 5 values in maxes, check if the current magnitude is greater than the smallest one in maxes list\n",
        "    # if so append\n",
        "    elif rec_centroid[0, col] > maxes[len(maxes)-1][1]:\n",
        "      maxes[len(maxes)-1] = [col, rec_centroid[0, col]]\n",
        "      maxes.sort(key=second_elem, reverse=True)\n",
        "  return maxes\n"
      ],
      "metadata": {
        "id": "a0fjcUqpt20r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our vectorizer again."
      ],
      "metadata": {
        "id": "-gsgpA-nzKQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer = spacy_pipeline)\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "Z2LmM2Efy3YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then iterate through our centroids finding the top 5 magnitudes for each."
      ],
      "metadata": {
        "id": "GUM6S-7izMgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(k):\n",
        "  maxes = five_maxes(rec_centroids[i], k)\n",
        "  max_terms = []\n",
        "  # find the actual terms that have the high magnitudes\n",
        "  for max in maxes:\n",
        "    max_terms.append([vectorizer.get_feature_names_out()[max[0]], max[1]])\n",
        "\n",
        "  print(max_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGZsGqDmt4JO",
        "outputId": "f6764557-0c5b-4bad-eeec-a6b589cc6efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['game', 0.07429247140526307], ['heat', 0.052523823893616366], ['laker', 0.04390201429783622], ['final', 0.04321583022745221], ['nba', 0.037732982529937675]]\n",
            "[['good', 0.08795186632143101], ['player', 0.02920071365269986], ['time', 0.028479433422451118], ['great', 0.018762881757477406], ['bam', 0.01857516419344807]]\n",
            "[['herro', 0.053054372854072274], ['jimmy', 0.05086146294566495], ['tyler', 0.039160341898404415], ['üòÇ', 0.03547664279140322], ['butler', 0.03311515883286048]]\n",
            "[['team', 0.07124559002642394], ['love', 0.03211651600473568], ['podcast', 0.02159350592628673], ['murray', 0.020847178457307162], ['nugget', 0.018334599764601816]]\n",
            "[['üî•', 0.11043202073074147], ['let', 0.09978538202567205], ['win', 0.048848666669937583], ['@miamiheat', 0.04876991577260555], ['üèÄ', 0.04347465642890756]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also see the label distribution."
      ],
      "metadata": {
        "id": "F-DJoXFi6Z0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define 5 lists of cluster labels\n",
        "clusters_labels = [[], [], [], [], []]\n",
        "for tweet in nbatweets:\n",
        "  if tweet['cluster'] == 0:\n",
        "    clusters_labels[0].append(tweet['team'])\n",
        "  elif tweet['cluster'] == 1:\n",
        "    clusters_labels[1].append(tweet['team'])\n",
        "  elif tweet['cluster'] == 2:\n",
        "    clusters_labels[2].append(tweet['team'])\n",
        "  elif tweet['cluster'] == 3:\n",
        "    clusters_labels[3].append(tweet['team'])\n",
        "  else:\n",
        "    clusters_labels[4].append(tweet['team'])\n",
        "\n",
        "# print the clusters label count\n",
        "print(pd.Series(clusters_labels[0]).value_counts())\n",
        "print(pd.Series(clusters_labels[1]).value_counts())\n",
        "print(pd.Series(clusters_labels[2]).value_counts())\n",
        "print(pd.Series(clusters_labels[3]).value_counts())\n",
        "print(pd.Series(clusters_labels[4]).value_counts())"
      ],
      "metadata": {
        "id": "3Q6uIlVt6ZlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write dictionary to a file to keep the cluster data.\n"
      ],
      "metadata": {
        "id": "-WIrQGciilEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "keys = nbatweets[0].keys()\n",
        "\n",
        "with open('nbatweets_kmeans.csv', 'w', newline='') as output_file:\n",
        "    dict_writer = csv.DictWriter(output_file, keys)\n",
        "    dict_writer.writeheader()\n",
        "    dict_writer.writerows(nbatweets)"
      ],
      "metadata": {
        "id": "iUXkoqCdipBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}